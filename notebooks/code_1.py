# -*- coding: utf-8 -*-
"""code-1 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QD_o7rDomsI0fsYEyw22w6ODHjQdaE9H
"""

import pandas as pd
import numpy as np
import string
import nltk
from nltk.corpus import stopwords
from nltk.util import ngrams
from nltk import word_tokenize
from nltk.stem.porter import *
nltk.download('punkt')
nltk.download('stopwords')
!pip install git+https://github.com/LIAAD/yake
import yake
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

df_train = pd.read_csv('news-train-1.csv')
df_train

df_test = pd.read_csv('news-test.csv')
df_test

"""<h2>Question 1</h2>"""

# PRE-PROCESS DATA

# stemming tool from nltk
stemmer = PorterStemmer()
# a mapping dictionary that help remove punctuations
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)
def get_tokens(text):
  # turn document into lowercase
  lowers = text.lower()
  # remove punctuations
  no_punctuation = lowers.translate(remove_punctuation_map)
  # tokenize document
  tokens = nltk.word_tokenize(no_punctuation)
  # remove stop words
  filtered = [w for w in tokens if not w in stopwords.words('english')]
  # stemming process
  stemmed = []
  for item in filtered:
      stemmed.append(stemmer.stem(item))
  # final unigrams
  return stemmed

tokens = []
for i in range(len(df_train['Text'])):
  tokens.append(get_tokens(df_train['Text'][i]))
flatten_tokens = [' '.join(i) for i in tokens]
# flatten_tokens

# N-GRAMS

n_grams = ngrams(sequence=word_tokenize(flatten_tokens[0]), n = 2)
for i in n_grams:
  print(i)

txt = df_train['Category'][0] + ", "+ df_train['Text'][0]

# KEYWORD EXTRACTION (USING YAKE)

kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)
keywords = kw_extractor.extract_keywords(txt)
for kw, v in keywords:
  print("Keyphrase: ",kw, ": score", v)

# KEYWORD EXTRACTION (USING SUMMA)

!pip install summa
from summa import keywords

TR_keywords = keywords.keywords(txt, scores=True)
print(TR_keywords[0:10])

# COUNT VECTORIZER

X = flatten_tokens
y = df_train['Category']

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
vectorizer = CountVectorizer(max_features=10000)
X = vectorizer.fit_transform(X).toarray()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def create_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    return model

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_score = []
val_l = []
val_a = []
TL = []
TA = []
for i, j in kf.split(X, y):
    X_train_fold, X_val_fold = X[i], X[j]
    y_train_fold, y_val_fold = y[i], y[j]
    model = create_model()
    temp = model.fit(X_train_fold, y_train_fold, epochs=5, batch_size=32, verbose=0)
    vl, va = model.evaluate(X_val_fold, y_val_fold)
    # acc_score.append(temp_acc)
    val_l.append(vl)
    val_a.append(va)
    TL.append(temp.history['loss'])
    TA.append(temp.history['accuracy'])


models_to_accuracies = {
    'CountVectorizer' :
    {
        'Average Training Accuracy': np.mean(TA),
        'Average Validation Accuracy': np.mean(val_a),
        'Average Training Loss': np.mean(TL),
        'Average Validation Loss': np.mean(val_l)
    }
}

# TFIDF VECTORIZER

X = flatten_tokens
y = df_train['Category']
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
vectorizer = TfidfVectorizer(max_features=10000)
X = vectorizer.fit_transform(X).toarray()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def create_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    return model

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_score = []
acc_score = []
val_l = []
val_a = []
TL = []
TA = []
for i, j in kf.split(X, y):
    X_train_fold, X_val_fold = X[i], X[j]
    y_train_fold, y_val_fold = y[i], y[j]
    model = create_model()
    temp = model.fit(X_train_fold, y_train_fold, epochs=5, batch_size=32, verbose=0)
    vl, va = model.evaluate(X_val_fold, y_val_fold)
    val_l.append(vl)
    val_a.append(va)
    TL.append(temp.history['loss'])
    TA.append(temp.history['accuracy'])

models_to_accuracies['TFIDF VECTORIZER'] = {
        'Average Training Accuracy': np.mean(TA),
        'Average Validation Accuracy': np.mean(val_a),
        'Average Training Loss': np.mean(TL),
        'Average Validation Loss': np.mean(val_l)
}

# GloVe with gensim


glove_input_file = 'glove.6B.50d.txt'
glove2word2vec(glove_input_file, 'out.txt')
glove_model = KeyedVectors.load_word2vec_format('out.txt', binary=False)

word_vectors = []
for text in flatten_tokens:
    tokens = text.split()
    vectors = [glove_model[word] if word in glove_model else np.zeros(glove_model.vector_size) for word in tokens]
    mean_vector = np.mean(vectors, axis=0)
    word_vectors.append(mean_vector)

X_glove = np.array(word_vectors)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df_train['Category'])
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(X_glove.shape[1],)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

n = 5
train_loss = []
train_acc = []
val_l = []
val_a = []
kf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)
for i, j in kf.split(X_glove, y_encoded):
    X_train, X_val = X_glove[i], X_glove[j]
    y_train, y_val = y_encoded[i], y_encoded[j]
    history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=False)
    val_loss, val_accuracy = model.evaluate(X_val, y_val)
    val_l.append(val_loss)
    val_a.append(val_accuracy)
    train_loss.append(history.history['loss'])
    train_acc.append(history.history['accuracy'])

models_to_accuracies['GloVe with gensim'] = {
        'Average Training Accuracy': np.mean(train_acc),
        'Average Validation Accuracy': np.mean(val_a),
        'Average Training Loss': np.mean(train_loss),
        'Average Validation Loss': np.mean(val_l)
}

# for fold, (train_loss, val_loss, train_accuracy, val_accuracy) in enumerate(zip(train_loss, val_loss, train_acc, val_acc)):
#     print(f'Fold {fold + 1} - Training Loss: {train_loss[-1]:.4f}, Training Accuracy: {train_accuracy[-1]:.4f}')
#     print(f'          Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

# Commented out IPython magic to ensure Python compatibility.
# BERT

# %pip install bert-tensorflow
# %pip install transformers
from transformers import BertTokenizer, TFBertModel, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertModel.from_pretrained('bert-base-uncased')

def text_to_bert_embedding(doc_text):
  input_ids = tokenizer.encode(doc_text, add_special_tokens=True, return_tensors='tf', padding=True, truncation=True)
  outputs = model(input_ids)
  hidden_states = outputs.last_hidden_state
  mean_embedding = tf.reduce_mean(hidden_states, axis=1)
  mean_embedding = mean_embedding.numpy()
  return mean_embedding.flatten()
x_bert = [text_to_bert_embedding(i) for i in flatten_tokens]
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(df_train['Category'])

# def text_to_bert_embedding(doc_text):

#   input_ids = tokenizer.encode(doc_text, add_special_tokens=True, return_tensors='tf', padding=True, truncation=True)
#   outputs = bert_model(input_ids)
#   hidden_states = outputs.last_hidden_state
#   mean_embedding = tf.reduce_mean(hidden_states, axis=1)
#   mean_embedding = mean_embedding.numpy()
#   return mean_embedding.flatten()

print(np.array(x_bert).shape)
print(y_train.shape)

def create_model():
    model = None
    model = Sequential()
    model.add(Dense(128, input_dim=ttt.shape[1], activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    return model


    # model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    # model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_score = []
train_loss = []
train_acc = []
val_l = []
val_a = []
ttt = np.array(x_bert)
for i, j in kf.split(ttt, y_train):
    X_train_fold, X_val_fold = ttt[i], ttt[j]
    y_train_fold, y_val_fold = y_train[i], y_train[j]
    model = create_model()
    temp = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)
    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold)
    val_l.append(val_loss)
    val_a.append(val_accuracy)
    train_loss.append(temp.history['loss'])
    train_acc.append(temp.history['accuracy'])


models_to_accuracies['BERT'] = {
        'Average Training Accuracy': np.mean(train_acc),
        'Average Validation Accuracy': np.mean(val_a),
        'Average Training Loss': np.mean(train_loss),
        'Average Validation Loss': np.mean(val_l)
}

results_table = pd.DataFrame(models_to_accuracies).T

results_table

import matplotlib.pyplot as plt
training_accuracies = list(results_table['Average Training Accuracy'])
validation_accuracies = list(results_table['Average Validation Accuracy'])
models = ['CountVectorizer', 'TFIDF VECTORIZER', 'GloVe with gensim', 'BERT']

data = pd.DataFrame({
    'model' : models,
    'training_accuracies':  training_accuracies,
    'validation_accuracies': validation_accuracies
})

cv_acc = list(data[data['model'] == 'CountVectorizer'].values[0][1:])
tf_acc = list(data[data['model'] == 'TFIDF VECTORIZER'].values[0][1:])
gl_acc = list(data[data['model'] == 'GloVe with gensim'].values[0][1:])
br_acc = list(data[data['model'] == 'BERT'].values[0][1:])

barWidth = 0.25
fig = plt.subplots(figsize=(10,6))

bar_1 = np.arange(len(gl_acc))
bar_2 = [x + barWidth for x in bar_1]
bar_3 = [x + barWidth for x in bar_2]
bar_4 = [x + barWidth for x in bar_3]

plt.bar(bar_1, cv_acc , color ='blue', width = barWidth,
        label ='CountVectorizer')
plt.bar(bar_2, tf_acc , color ='orange', width = barWidth,
        label ='TFIDFVectorizer')
plt.bar(bar_3, gl_acc , color ='green', width = barWidth,
        label ='Glove_with_gensim')
plt.bar(bar_4, br_acc , color ='red', width = barWidth,
        label ='BERT')


plt.xlabel('Criterion')
plt.ylabel('Accuracy')
plt.xticks([r + barWidth for r in range(2)],
        ['Training Accuracy', 'Testing Accuracy'])

plt.legend()

# COUNT VECTORIZER

X = flatten_tokens
y = df_train['Category']

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
vectorizer = TfidfVectorizer(max_features=10000)
X = vectorizer.fit_transform(X).toarray()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def create_model(l):
    model = Sequential()
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=l), metrics=['accuracy'])
    return model

learning_rates =  [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1]
best_lr = None
best_accuracy = 0

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_score = []
val_l = []
val_a = []
TL = []
TA = []
training_std = []
validation_std = []


for lr in learning_rates:
    training_losses_per = []
    training_accuracies_per = []
    validation_losses_per = []
    validation_accuracies_per = []
    for i, j in kf.split(X, y):
        X_train_fold, X_val_fold = X[i], X[j]
        y_train_fold, y_val_fold = y[i], y[j]
        model = create_model(lr)
        temp = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)
        vl, va = model.evaluate(X_val_fold, y_val_fold)
        validation_losses_per.append(vl)
        validation_accuracies_per.append(va)
        training_losses_per.append(temp.history['loss'])
        training_accuracies_per.append(temp.history['accuracy'])
    val_a.append(np.mean(validation_accuracies_per))
    val_l.append(np.mean(validation_losses_per))
    TA.append(np.mean(training_accuracies_per))
    TL.append(np.mean(training_losses_per))
    training_std.append(np.std(training_accuracies_per))
    validation_std.append(np.std(validation_accuracies_per))
    mean_accuracy = np.mean(validation_accuracies_per)
    if mean_accuracy > best_accuracy:
        best_accuracy = mean_accuracy
        best_lr = lr

TA

val_a

results_df = {
    'Learning Rates': learning_rates,
    'Average Training Accuracies': TA,
    'Average Validation Accuracies': val_a,
    'Training Standard Deviation' : training_std,
    'Validation Standard Deviation' : validation_std,
}

results_df = pd.DataFrame(results_df)

results_df

plt.figure(figsize=(10, 6))
plt.plot(learning_rates, TA, marker='o', label='Training Accuracy')
plt.plot(learning_rates, val_a, marker='o', label='Validation Accuracy')

plt.title('Training and Validation Accuracy vs. Learning Rate')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy')
plt.xscale('log')
plt.legend()
plt.show()

X = flatten_tokens
y = df_train['Category']
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
vectorizer = TfidfVectorizer(max_features=10000)
X = vectorizer.fit_transform(X).toarray()

optimizers = [ 'SGD',"RMSprop", 'Adam']

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df_train['Category'])

num_folds = 5

validation_losses = []
validation_accuracies = []
training_losses = []
training_accuracies = []
training_std = []
validation_std = []

best_optimizer = ''
best_accuracy = 0.0
best_loss = 100.0

kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

for op in optimizers:
    training_losses_per = []
    training_accuracies_per = []
    validation_losses_per = []
    validation_accuracies_per = []

    model_6 = tf.keras.Sequential([
            tf.keras.layers.Input(shape=(X.shape[1],)),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(5, activation='softmax')
            ])

    for train_indices, val_indices in kf.split(X, y):
        X_train, X_val = X[train_indices], X[val_indices]
        y_train, y_val = y_encoded[train_indices], y_encoded[val_indices]

        if op == 'SGD':
            optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
        elif op == 'Adam':
            optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
        elif op == 'RMSprop':
            optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)

        model_6.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        history = model_6.fit(X_train, y_train, epochs=10, batch_size=32, verbose=False)

        val_loss, val_accuracy = model_6.evaluate(X_val, y_val)
        validation_losses_per.append(val_loss)
        validation_accuracies_per.append(val_accuracy)

        training_losses_per.append(history.history['loss'])
        training_accuracies_per.append(history.history['accuracy'])
    validation_accuracies.append(np.mean(validation_accuracies_per))
    validation_losses.append(np.mean(validation_losses_per))
    training_accuracies.append(np.mean(training_accuracies_per))
    training_losses.append(np.mean(training_losses_per))
    training_std.append(np.std(training_accuracies_per))
    validation_std.append(np.std(validation_accuracies_per))

results_df = {
    'Optimizers': optimizers,
    'Average Training Accuracies': training_accuracies,
    'Average Validation Accuracies': validation_accuracies,
    'Training Standard Deviation' : training_std,
    'Validation Standard Deviation' : validation_std,
}

results_df = pd.DataFrame(results_df)

results_df

import matplotlib.pyplot as plt

barWidth = 0.25
fig = plt.subplots(figsize=(16,10))

bar_1 = np.arange(3)
bar_2 = [x + barWidth for x in bar_1]
bar_3 = [x + barWidth for x in bar_2]

plt.bar(bar_1, training_accuracies , color ='blue', width = barWidth,
        label ='Training Accuracy')
plt.bar(bar_2, validation_accuracies , color ='orange', width = barWidth,
        label ='Validation Accuracy')

plt.xlabel('Optimizers')
plt.ylabel('Accuracy')
plt.xticks([r + barWidth/2 for r in range(3)],
        optimizers)

plt.legend()

X_train = df_train['Text']
y_train = df_train['Category']

# Use LabelEncoder to convert categories to numerical labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)

# Initialize CountVectorizer
vectorizer = TfidfVectorizer(max_features=10000)
X_train = vectorizer.fit_transform(X_train).toarray()
X_test = vectorizer.transform(df_test['Text']).toarray()

# Neural network model
def create_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    return model

# 5-fold cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
for train_index, val_index in kf.split(X_train, y_train):
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    model = create_model()
    model.fit(X_train_fold, y_train_fold, epochs=5, batch_size=32, verbose=0)
    _, accuracy = model.evaluate(X_val_fold, y_val_fold)
    accuracies.append(accuracy)

# Average accuracy across folds
average_accuracy = np.mean(accuracies)
print("Average Accuracy:", average_accuracy)

# Training the final model
final_model = create_model()
final_model.fit(X_train, y_train, epochs=5, batch_size=32)

# Make predictions on the test data
test_predictions = final_model.predict(X_test)
test_predictions = np.argmax(test_predictions, axis=1)
test_predictions = label_encoder.inverse_transform(test_predictions)

# Save the predictions to a CSV file
df_test['Category'] = test_predictions
df_test.to_csv('news_test_predictions.csv', index=False)

df3 = pd.DataFrame(df_test['Category'])
df3

df3.to_csv('df3', index = False)

tokens_2 = []
for i in range(len(df_test['Text'])):
  tokens_2.append(get_tokens(df_test['Text'][i]))
flatten_tokens_2 = [' '.join(i) for i in tokens_2]
# flatten_tokens

X_train = flatten_tokens
y_train = df_train['Category']

# Use LabelEncoder to convert categories to numerical labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)

# Initialize CountVectorizer
vectorizer = TfidfVectorizer(max_features=10000)
X_train = vectorizer.fit_transform(X_train).toarray()
X_test = vectorizer.transform(flatten_tokens_2).toarray()

# Neural network model
def create_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    return model

# 5-fold cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
for train_index, val_index in kf.split(X_train, y_train):
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    model = create_model()
    model.fit(X_train_fold, y_train_fold, epochs=5, batch_size=32, verbose=0)
    _, accuracy = model.evaluate(X_val_fold, y_val_fold)
    accuracies.append(accuracy)

# Average accuracy across folds
average_accuracy = np.mean(accuracies)
print("Average Accuracy:", average_accuracy)

# Training the final model
final_model = create_model()
final_model.fit(X_train, y_train, epochs=5, batch_size=32)

# Make predictions on the test data
test_predictions = final_model.predict(X_test)
test_predictions = np.argmax(test_predictions, axis=1)
test_predictions = label_encoder.inverse_transform(test_predictions)

# Save the predictions to a CSV file
df_test['Category'] = test_predictions
df_test.to_csv('news_test_predictions.csv', index=False)

df3 = pd.DataFrame(df_test['Category'])
df3.to_csv('labels.csv', index = False)